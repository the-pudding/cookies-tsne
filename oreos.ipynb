{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rY-YFJUIuSlL",
    "outputId": "759077ac-5617-437e-978c-b6f58afbcc3e"
   },
   "outputs": [],
   "source": [
    "# Not all of these are needed, but most are (this cell is an artefact of a previous analysis). \n",
    "# If you don't have some of these python libraries, the easiest way to install them is using\n",
    "# \n",
    "# > pip install library_name\n",
    "# \n",
    "# Alternatively, you can do so using Anaconda, using \n",
    "# \n",
    "# > conda install library_name\n",
    "\n",
    "\n",
    "import string\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import sys\n",
    "# stdout = sys.stdout\n",
    "# sys.reload(sys)\n",
    "# sys.setdefaultencoding('utf-8')\n",
    "# sys.stdout = stdout\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "# import jupyternotify\n",
    "# ip = get_ipython()\n",
    "# ip.register_magics(jupyternotify.JupyterNotifyMagics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/iblinderman/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/iblinderman/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cq9Nby2EuSlV",
    "outputId": "d37ee235-d361-45d9-bc7c-edb10cb68df5",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sandwich cookie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>black-and-white cookie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>popular cookie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sweet sandwich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>creme-filled cookie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nabisco cookie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cookie favorite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>layered cookie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>black-and-white treat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>two-tone treat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>two-tone cookie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hydrox rival</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>triple-decker cookie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>black-and-white cookies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>nabisco treat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>cream-filled cookie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>twistable cookie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>best-selling cookie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sandwich cookies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>twistable treat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>black-and-white snack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>cookie since 1912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>creme cookie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>cookie brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>lunchbox treat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>dunkable treat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>three-layer cookie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>classic cookie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>cookie many take apart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>milk's favorite cookie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1269</th>\n",
       "      <td>crunchy treats with milk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270</th>\n",
       "      <td>cookie choices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271</th>\n",
       "      <td>cookies in a box lunch, sometimes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>some black-and-white snacks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273</th>\n",
       "      <td>twisted-apart cookies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>twistable snacks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>wonderfilled cookies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1276</th>\n",
       "      <td>cookies deep-fried at fairs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>cookies in mcflurrys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>black-and-white sleeveful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>dunkables since 1912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>black-white-black snacks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>cookies in sleeves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1282</th>\n",
       "      <td>cream-filled chocolate snacks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283</th>\n",
       "      <td>sandwich snacks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284</th>\n",
       "      <td>double stufs, eg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1285</th>\n",
       "      <td>droxies lookalikes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1286</th>\n",
       "      <td>stackable snacks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1287</th>\n",
       "      <td>ornately embossed edibles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1288</th>\n",
       "      <td>ingredients in tgi friday's dessert cup of dirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1289</th>\n",
       "      <td>popular snacks that inspired this puzzle's theme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1290</th>\n",
       "      <td>lunchbox favorites</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1291</th>\n",
       "      <td>crunchy pie crust components</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1292</th>\n",
       "      <td>sandwiches in lunchboxes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293</th>\n",
       "      <td>sweet snacks for over a century</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>makeup of some pie crusts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1295</th>\n",
       "      <td>tripartite treats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296</th>\n",
       "      <td>crumbles on sundaes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1297</th>\n",
       "      <td>treats once advertised as \"the original twister\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>three-part snacks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1299 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  clue\n",
       "0                                      sandwich cookie\n",
       "1                               black-and-white cookie\n",
       "2                                       popular cookie\n",
       "3                                       sweet sandwich\n",
       "4                                  creme-filled cookie\n",
       "...                                                ...\n",
       "1294                         makeup of some pie crusts\n",
       "1295                                 tripartite treats\n",
       "1296                               crumbles on sundaes\n",
       "1297  treats once advertised as \"the original twister\"\n",
       "1298                                 three-part snacks\n",
       "\n",
       "[1299 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading and cleaning the data structure. For this example, I'm using StackOverflow posts; specifically, \n",
    "# posts to the \"Interpersonal questions\" site on StackExchange. There's a bit of cleaning that's necessary here, \n",
    "# Which I take care of in this cell.\n",
    "\n",
    "df_stack = pd.read_csv(\"oreo-clues.tsv\", sep='\\t')\n",
    "df_stack\n",
    "# df_stack_qs = df_stack[df_stack['PostTypeId']==1]\n",
    "# from HTMLParser import HTMLParser\n",
    "\n",
    "# class MLStripper(HTMLParser):\n",
    "#     def __init__(self):\n",
    "#         self.reset()\n",
    "#         self.fed = []\n",
    "#     def handle_data(self, d):\n",
    "#         self.fed.append(d)\n",
    "#     def get_data(self):\n",
    "#         return ''.join(self.fed)\n",
    "\n",
    "# def strip_tags(html):\n",
    "#     s = MLStripper()\n",
    "#     s.feed(html)\n",
    "#     return s.get_data()\n",
    "\n",
    "# df_stack_qs = df_stack_qs.replace(r'\\n','', regex=True) \n",
    "\n",
    "# df_stack_qs['Body']=  df_stack_qs['Body'].apply(lambda x: strip_tags(x)) \n",
    "\n",
    "# Here's what the end product looks like\n",
    "# df_stack_qs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4F7P3KapuSlb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>sandwich cookie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>black-and-white cookie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>popular cookie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>sweet sandwich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>creme-filled cookie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   title                question\n",
       "0      0         sandwich cookie\n",
       "1      1  black-and-white cookie\n",
       "2      2          popular cookie\n",
       "3      3          sweet sandwich\n",
       "4      4     creme-filled cookie"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stack = df_stack.reset_index()\n",
    "df_stack.columns = ['title','question']\n",
    "df_stack.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TjucVqbouSld"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>1052</td>\n",
       "      <td>cookie that some people eat with mustard</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      title                                  question\n",
       "1052   1052  cookie that some people eat with mustard"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stack[df_stack['question'].str.contains('musta')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "22RZNM-auSlg"
   },
   "outputs": [],
   "source": [
    "# Creating lists of questions + their relevant titles\n",
    "questions =  df_stack['question'].tolist()\n",
    "titles =  df_stack['title'].tolist()\n",
    "\n",
    "\n",
    "# Adding title and data to a dictionary\n",
    "tempDict = {}\n",
    "for title, question in zip(titles, questions):\n",
    "    tempDict[title]=question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eSqrijYSuSlj",
    "outputId": "ae18e971-b2c8-463d-96ea-04b8170cda98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating tf-idf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reducing tf-idf to dimensions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Removing punctuation from the text, as well as some misc. irrelevant characters \n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower() # lower case\n",
    "    for e in set(string.punctuation+'\\n'+'\\t'): # remove punctuation and line breaks/tabs\n",
    "        text = text.replace(e, ' ')\t\n",
    "    for i in range(0,10):\t# remove double spaces\n",
    "        text = text.replace('  ', ' ')\n",
    "    text = text.translate(string.punctuation)  # punctuation\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    text = [w for w in tokens if not w in stopwords.words('english')] # stopwords\n",
    "    stems = []\n",
    "    for item in tokens: # stem\n",
    "        stems.append(wordnet_lemmatizer.lemmatize(item))\n",
    "    return stems\n",
    "\n",
    "# calculate tfidf \n",
    "print(\"calculating tf-idf\")\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english', min_df=0.025, max_df=.5) #NOTE\n",
    "# this step takes longest & contains lots of important parameters; playing with these and experimenting\n",
    "# with them is recommended. Starting here::\n",
    "# https://www.kaggle.com/adamschroeder/countvectorizer-tfidfvectorizer-predict-comments\n",
    "# and moving on to the official docs here:\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer\n",
    "# is a good idea.\n",
    "\n",
    "tfs = tfidf.fit_transform(tempDict.values())\n",
    "print(\"reducing tf-idf to dimensions\")\n",
    "tfs_reduced = TruncatedSVD(n_components=5, random_state=0).fit_transform(tfs)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DmGISjzkuSlo",
    "outputId": "f0b436ca-c604-4022-c07e-8b1933270ec2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing pairwise distances...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 1299\n",
      "[t-SNE] Computed conditional probabilities for sample 1299 / 1299\n",
      "[t-SNE] Mean sigma: 0.000000\n",
      "[t-SNE] Iteration 50: error = 50.4335849, gradient norm = 0.0675093 (50 iterations in 3.316s)\n",
      "[t-SNE] Iteration 100: error = 46.6442038, gradient norm = 0.0305864 (50 iterations in 3.158s)\n",
      "[t-SNE] Iteration 150: error = 45.4378952, gradient norm = 0.0155149 (50 iterations in 3.219s)\n",
      "[t-SNE] Iteration 200: error = 45.2679894, gradient norm = 0.0145035 (50 iterations in 3.197s)\n",
      "[t-SNE] Iteration 250: error = 45.0659055, gradient norm = 0.0093343 (50 iterations in 3.256s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 45.065906\n",
      "[t-SNE] Iteration 300: error = 1.1063509, gradient norm = 0.0017769 (50 iterations in 3.204s)\n",
      "[t-SNE] Iteration 350: error = 1.1890628, gradient norm = 0.0004607 (50 iterations in 3.240s)\n",
      "[t-SNE] Iteration 400: error = 1.2215307, gradient norm = 0.0001004 (50 iterations in 3.227s)\n",
      "[t-SNE] Iteration 450: error = 1.1681552, gradient norm = 0.0000549 (50 iterations in 3.222s)\n",
      "[t-SNE] Iteration 500: error = 1.2020246, gradient norm = 0.0000259 (50 iterations in 3.285s)\n",
      "[t-SNE] Iteration 550: error = 1.2260380, gradient norm = 0.0000149 (50 iterations in 3.243s)\n",
      "[t-SNE] Iteration 600: error = 1.2410281, gradient norm = 0.0000099 (50 iterations in 3.290s)\n",
      "[t-SNE] Iteration 650: error = 1.2492456, gradient norm = 0.0000077 (50 iterations in 3.196s)\n",
      "[t-SNE] Iteration 650: did not make any progress during the last 300 episodes. Finished.\n",
      "[t-SNE] KL divergence after 650 iterations: 1.249246\n"
     ]
    }
   ],
   "source": [
    "model = TSNE(n_components=5, perplexity=5, verbose=2, method='exact').fit_transform(tfs_reduced)\n",
    "\n",
    "# save to json file\n",
    "x_axis=model[:,0]\n",
    "y_axis=model[:,1]\n",
    "x_norm = (x_axis-np.min(x_axis)) / (np.max(x_axis) - np.min(x_axis))\n",
    "y_norm = (y_axis-np.min(y_axis)) / (np.max(y_axis) - np.min(y_axis))\n",
    "data = {\"x\":x_norm.tolist(), \"y\":y_norm.tolist(), \"names\":tempDict.keys()} #output x and y coords in data\n",
    "with open('test.json', 'w') as outfile:\n",
    "    json.dump(list(data), outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ni6mVt8ouSlr"
   },
   "outputs": [],
   "source": [
    "# Importing json of results, merging it with the original file, and outputting a CSV which will contain\n",
    "# the X and Y coords of each point, which we'll use to create our tSNE plot\n",
    "# df_xyplot = pd.read_json(\"test.json\")\n",
    "\n",
    "\n",
    "\n",
    "test_df_coords = pd.DataFrame(data)[['x','y']].reset_index()\n",
    "test_df_coords.columns = ['names','x','y']\n",
    "\n",
    "result = pd.merge(df_stack, test_df_coords, left_on='title', right_on='names')\n",
    "result.to_csv(\"oreos_tsn_reduced_sample_v1_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IqxPq7xguSlu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reducing tf-idf to dimensions\n",
      "done\n",
      "[t-SNE] Computing pairwise distances...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 1299\n",
      "[t-SNE] Computed conditional probabilities for sample 1299 / 1299\n",
      "[t-SNE] Mean sigma: 0.000000\n",
      "[t-SNE] Iteration 50: error = 50.6768160, gradient norm = 0.0265493 (50 iterations in 4.169s)\n",
      "[t-SNE] Iteration 100: error = 48.2324722, gradient norm = 0.0108722 (50 iterations in 4.002s)\n",
      "[t-SNE] Iteration 150: error = 47.5962654, gradient norm = 0.0102426 (50 iterations in 4.115s)\n",
      "[t-SNE] Iteration 200: error = 47.2916364, gradient norm = 0.0042732 (50 iterations in 4.064s)\n",
      "[t-SNE] Iteration 250: error = 47.0882932, gradient norm = 0.0025209 (50 iterations in 4.074s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 47.088293\n",
      "[t-SNE] Iteration 300: error = 4.8111344, gradient norm = 0.0003089 (50 iterations in 4.498s)\n",
      "[t-SNE] Iteration 350: error = 4.4628243, gradient norm = 0.0000407 (50 iterations in 4.150s)\n",
      "[t-SNE] Iteration 400: error = 4.3002388, gradient norm = 0.0000179 (50 iterations in 4.147s)\n",
      "[t-SNE] Iteration 450: error = 4.1994212, gradient norm = 0.0000102 (50 iterations in 4.147s)\n",
      "[t-SNE] Iteration 500: error = 4.1252850, gradient norm = 0.0000071 (50 iterations in 4.394s)\n",
      "[t-SNE] Iteration 550: error = 4.0724705, gradient norm = 0.0000047 (50 iterations in 4.129s)\n",
      "[t-SNE] Iteration 600: error = 4.0300106, gradient norm = 0.0000036 (50 iterations in 4.172s)\n",
      "[t-SNE] Iteration 650: error = 3.9945101, gradient norm = 0.0000028 (50 iterations in 4.176s)\n",
      "[t-SNE] Iteration 700: error = 3.9641337, gradient norm = 0.0000023 (50 iterations in 4.491s)\n",
      "[t-SNE] Iteration 750: error = 3.9371801, gradient norm = 0.0000019 (50 iterations in 4.312s)\n",
      "[t-SNE] Iteration 800: error = 3.9129272, gradient norm = 0.0000016 (50 iterations in 4.076s)\n",
      "[t-SNE] Iteration 850: error = 3.8919262, gradient norm = 0.0000014 (50 iterations in 4.061s)\n",
      "[t-SNE] Iteration 900: error = 3.8732872, gradient norm = 0.0000012 (50 iterations in 4.137s)\n",
      "[t-SNE] Iteration 950: error = 3.8562980, gradient norm = 0.0000010 (50 iterations in 4.051s)\n",
      "[t-SNE] Iteration 1000: error = 3.8402478, gradient norm = 0.0000009 (50 iterations in 4.107s)\n",
      "[t-SNE] KL divergence after 1000 iterations: 3.840248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reducing tf-idf to dimensions\n",
      "done\n",
      "[t-SNE] Computing pairwise distances...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 1299\n",
      "[t-SNE] Computed conditional probabilities for sample 1299 / 1299\n",
      "[t-SNE] Mean sigma: 0.000000\n",
      "[t-SNE] Iteration 50: error = 50.1748531, gradient norm = 0.0298623 (50 iterations in 4.065s)\n",
      "[t-SNE] Iteration 100: error = 47.7006130, gradient norm = 0.0109433 (50 iterations in 3.933s)\n",
      "[t-SNE] Iteration 150: error = 47.0879828, gradient norm = 0.0046425 (50 iterations in 4.147s)\n",
      "[t-SNE] Iteration 200: error = 46.7915701, gradient norm = 0.0051323 (50 iterations in 4.269s)\n",
      "[t-SNE] Iteration 250: error = 46.6032948, gradient norm = 0.0050040 (50 iterations in 3.992s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 46.603295\n",
      "[t-SNE] Iteration 300: error = 4.8583763, gradient norm = 0.0002629 (50 iterations in 4.081s)\n",
      "[t-SNE] Iteration 350: error = 4.4811283, gradient norm = 0.0000408 (50 iterations in 4.628s)\n",
      "[t-SNE] Iteration 400: error = 4.3063487, gradient norm = 0.0000182 (50 iterations in 4.161s)\n",
      "[t-SNE] Iteration 450: error = 4.1972729, gradient norm = 0.0000103 (50 iterations in 4.024s)\n",
      "[t-SNE] Iteration 500: error = 4.1182120, gradient norm = 0.0000069 (50 iterations in 4.160s)\n",
      "[t-SNE] Iteration 550: error = 4.0599746, gradient norm = 0.0000047 (50 iterations in 4.038s)\n",
      "[t-SNE] Iteration 600: error = 4.0105509, gradient norm = 0.0000051 (50 iterations in 4.660s)\n",
      "[t-SNE] Iteration 650: error = 3.9719957, gradient norm = 0.0000029 (50 iterations in 4.147s)\n",
      "[t-SNE] Iteration 700: error = 3.9393204, gradient norm = 0.0000023 (50 iterations in 3.956s)\n",
      "[t-SNE] Iteration 750: error = 3.9130698, gradient norm = 0.0000019 (50 iterations in 4.006s)\n",
      "[t-SNE] Iteration 800: error = 3.8899060, gradient norm = 0.0000016 (50 iterations in 4.074s)\n",
      "[t-SNE] Iteration 850: error = 3.8682862, gradient norm = 0.0000016 (50 iterations in 4.654s)\n",
      "[t-SNE] Iteration 900: error = 3.8498346, gradient norm = 0.0000012 (50 iterations in 4.657s)\n",
      "[t-SNE] Iteration 950: error = 3.8341937, gradient norm = 0.0000010 (50 iterations in 4.737s)\n",
      "[t-SNE] Iteration 1000: error = 3.8195666, gradient norm = 0.0000009 (50 iterations in 4.550s)\n",
      "[t-SNE] KL divergence after 1000 iterations: 3.819567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reducing tf-idf to dimensions\n",
      "done\n",
      "[t-SNE] Computing pairwise distances...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 1299\n",
      "[t-SNE] Computed conditional probabilities for sample 1299 / 1299\n",
      "[t-SNE] Mean sigma: 0.000000\n",
      "[t-SNE] Iteration 50: error = 51.8811464, gradient norm = 0.0310574 (50 iterations in 4.305s)\n",
      "[t-SNE] Iteration 100: error = 50.6216795, gradient norm = 0.0146580 (50 iterations in 4.512s)\n",
      "[t-SNE] Iteration 150: error = 50.3927565, gradient norm = 0.0064963 (50 iterations in 4.360s)\n",
      "[t-SNE] Iteration 200: error = 50.3634547, gradient norm = 0.0043554 (50 iterations in 4.430s)\n",
      "[t-SNE] Iteration 250: error = 50.4651920, gradient norm = 0.0034739 (50 iterations in 4.833s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 50.465192\n",
      "[t-SNE] Iteration 300: error = 5.0469498, gradient norm = 0.0003140 (50 iterations in 4.464s)\n",
      "[t-SNE] Iteration 350: error = 4.6754025, gradient norm = 0.0000393 (50 iterations in 4.461s)\n",
      "[t-SNE] Iteration 400: error = 4.5005693, gradient norm = 0.0000178 (50 iterations in 4.628s)\n",
      "[t-SNE] Iteration 450: error = 4.3888459, gradient norm = 0.0000101 (50 iterations in 4.725s)\n",
      "[t-SNE] Iteration 500: error = 4.3099347, gradient norm = 0.0000066 (50 iterations in 4.467s)\n",
      "[t-SNE] Iteration 550: error = 4.2472218, gradient norm = 0.0000048 (50 iterations in 4.159s)\n",
      "[t-SNE] Iteration 600: error = 4.1990231, gradient norm = 0.0000036 (50 iterations in 4.147s)\n",
      "[t-SNE] Iteration 650: error = 4.1597101, gradient norm = 0.0000028 (50 iterations in 4.134s)\n",
      "[t-SNE] Iteration 700: error = 4.1270092, gradient norm = 0.0000022 (50 iterations in 4.095s)\n",
      "[t-SNE] Iteration 750: error = 4.0978541, gradient norm = 0.0000019 (50 iterations in 4.207s)\n",
      "[t-SNE] Iteration 800: error = 4.0730085, gradient norm = 0.0000016 (50 iterations in 4.147s)\n",
      "[t-SNE] Iteration 850: error = 4.0512551, gradient norm = 0.0000014 (50 iterations in 4.076s)\n",
      "[t-SNE] Iteration 900: error = 4.0327980, gradient norm = 0.0000011 (50 iterations in 4.098s)\n",
      "[t-SNE] Iteration 950: error = 4.0153885, gradient norm = 0.0000010 (50 iterations in 4.101s)\n",
      "[t-SNE] Iteration 1000: error = 3.9994515, gradient norm = 0.0000009 (50 iterations in 4.178s)\n",
      "[t-SNE] KL divergence after 1000 iterations: 3.999452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reducing tf-idf to dimensions\n",
      "done\n",
      "[t-SNE] Computing pairwise distances...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 1299\n",
      "[t-SNE] Computed conditional probabilities for sample 1299 / 1299\n",
      "[t-SNE] Mean sigma: 0.000000\n",
      "[t-SNE] Iteration 50: error = 52.5095189, gradient norm = 0.0298421 (50 iterations in 4.198s)\n",
      "[t-SNE] Iteration 100: error = 51.2429737, gradient norm = 0.0217898 (50 iterations in 4.165s)\n",
      "[t-SNE] Iteration 150: error = 51.1697712, gradient norm = 0.0069945 (50 iterations in 4.108s)\n",
      "[t-SNE] Iteration 200: error = 51.3620644, gradient norm = 0.0036903 (50 iterations in 4.181s)\n",
      "[t-SNE] Iteration 250: error = 51.3661496, gradient norm = 0.0032426 (50 iterations in 4.120s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 51.366150\n",
      "[t-SNE] Iteration 300: error = 5.1077832, gradient norm = 0.0002438 (50 iterations in 4.054s)\n",
      "[t-SNE] Iteration 350: error = 4.7436109, gradient norm = 0.0000383 (50 iterations in 4.133s)\n",
      "[t-SNE] Iteration 400: error = 4.5678846, gradient norm = 0.0000177 (50 iterations in 4.109s)\n",
      "[t-SNE] Iteration 450: error = 4.4573369, gradient norm = 0.0000099 (50 iterations in 4.108s)\n",
      "[t-SNE] Iteration 500: error = 4.3808079, gradient norm = 0.0000065 (50 iterations in 4.098s)\n",
      "[t-SNE] Iteration 550: error = 4.3215357, gradient norm = 0.0000046 (50 iterations in 4.089s)\n",
      "[t-SNE] Iteration 600: error = 4.2733109, gradient norm = 0.0000035 (50 iterations in 4.157s)\n",
      "[t-SNE] Iteration 650: error = 4.2324260, gradient norm = 0.0000028 (50 iterations in 4.200s)\n",
      "[t-SNE] Iteration 700: error = 4.1974689, gradient norm = 0.0000022 (50 iterations in 4.202s)\n",
      "[t-SNE] Iteration 750: error = 4.1654012, gradient norm = 0.0000021 (50 iterations in 4.146s)\n",
      "[t-SNE] Iteration 800: error = 4.1391388, gradient norm = 0.0000016 (50 iterations in 4.142s)\n",
      "[t-SNE] Iteration 850: error = 4.1174509, gradient norm = 0.0000013 (50 iterations in 4.146s)\n",
      "[t-SNE] Iteration 900: error = 4.0975848, gradient norm = 0.0000011 (50 iterations in 4.126s)\n",
      "[t-SNE] Iteration 950: error = 4.0789499, gradient norm = 0.0000010 (50 iterations in 4.140s)\n",
      "[t-SNE] Iteration 1000: error = 4.0620964, gradient norm = 0.0000009 (50 iterations in 4.165s)\n",
      "[t-SNE] KL divergence after 1000 iterations: 4.062096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reducing tf-idf to dimensions\n",
      "done\n",
      "[t-SNE] Computing pairwise distances...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 1299\n",
      "[t-SNE] Computed conditional probabilities for sample 1299 / 1299\n",
      "[t-SNE] Mean sigma: 0.000000\n",
      "[t-SNE] Iteration 50: error = 50.4585029, gradient norm = 0.0352106 (50 iterations in 4.327s)\n",
      "[t-SNE] Iteration 100: error = 48.4710550, gradient norm = 0.0098859 (50 iterations in 4.820s)\n",
      "[t-SNE] Iteration 150: error = 47.9374773, gradient norm = 0.0081175 (50 iterations in 4.654s)\n",
      "[t-SNE] Iteration 200: error = 47.6054560, gradient norm = 0.0040622 (50 iterations in 4.545s)\n",
      "[t-SNE] Iteration 250: error = 47.7072972, gradient norm = 0.0026430 (50 iterations in 4.466s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 47.707297\n",
      "[t-SNE] Iteration 300: error = 5.0463543, gradient norm = 0.0002934 (50 iterations in 4.359s)\n",
      "[t-SNE] Iteration 350: error = 4.6577323, gradient norm = 0.0000404 (50 iterations in 4.321s)\n",
      "[t-SNE] Iteration 400: error = 4.4882062, gradient norm = 0.0000179 (50 iterations in 4.466s)\n",
      "[t-SNE] Iteration 450: error = 4.3833123, gradient norm = 0.0000100 (50 iterations in 4.623s)\n",
      "[t-SNE] Iteration 500: error = 4.3096682, gradient norm = 0.0000069 (50 iterations in 4.606s)\n",
      "[t-SNE] Iteration 550: error = 4.2543020, gradient norm = 0.0000047 (50 iterations in 4.369s)\n",
      "[t-SNE] Iteration 600: error = 4.2101058, gradient norm = 0.0000035 (50 iterations in 4.520s)\n",
      "[t-SNE] Iteration 650: error = 4.1731885, gradient norm = 0.0000028 (50 iterations in 4.779s)\n",
      "[t-SNE] Iteration 700: error = 4.1423806, gradient norm = 0.0000022 (50 iterations in 4.616s)\n",
      "[t-SNE] Iteration 750: error = 4.1141127, gradient norm = 0.0000020 (50 iterations in 4.297s)\n",
      "[t-SNE] Iteration 800: error = 4.0907481, gradient norm = 0.0000015 (50 iterations in 4.261s)\n",
      "[t-SNE] Iteration 850: error = 4.0699979, gradient norm = 0.0000013 (50 iterations in 4.405s)\n",
      "[t-SNE] Iteration 900: error = 4.0510256, gradient norm = 0.0000011 (50 iterations in 4.793s)\n",
      "[t-SNE] Iteration 950: error = 4.0335129, gradient norm = 0.0000010 (50 iterations in 4.381s)\n",
      "[t-SNE] Iteration 1000: error = 4.0178032, gradient norm = 0.0000009 (50 iterations in 4.284s)\n",
      "[t-SNE] KL divergence after 1000 iterations: 4.017803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reducing tf-idf to dimensions\n",
      "done\n",
      "[t-SNE] Computing pairwise distances...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 1299\n",
      "[t-SNE] Computed conditional probabilities for sample 1299 / 1299\n",
      "[t-SNE] Mean sigma: 0.000000\n",
      "[t-SNE] Iteration 50: error = 50.2624765, gradient norm = 0.0382067 (50 iterations in 4.350s)\n",
      "[t-SNE] Iteration 100: error = 47.3543594, gradient norm = 0.0142258 (50 iterations in 4.286s)\n",
      "[t-SNE] Iteration 150: error = 46.8681367, gradient norm = 0.0054000 (50 iterations in 4.211s)\n",
      "[t-SNE] Iteration 200: error = 46.7870574, gradient norm = 0.0051133 (50 iterations in 4.276s)\n",
      "[t-SNE] Iteration 250: error = 46.8985738, gradient norm = 0.0029820 (50 iterations in 4.345s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 46.898574\n",
      "[t-SNE] Iteration 300: error = 4.9651349, gradient norm = 0.0002603 (50 iterations in 4.227s)\n",
      "[t-SNE] Iteration 350: error = 4.6120800, gradient norm = 0.0000400 (50 iterations in 4.137s)\n",
      "[t-SNE] Iteration 400: error = 4.4481684, gradient norm = 0.0000174 (50 iterations in 4.306s)\n",
      "[t-SNE] Iteration 450: error = 4.3430730, gradient norm = 0.0000102 (50 iterations in 4.343s)\n",
      "[t-SNE] Iteration 500: error = 4.2666068, gradient norm = 0.0000069 (50 iterations in 4.253s)\n",
      "[t-SNE] Iteration 550: error = 4.2082938, gradient norm = 0.0000053 (50 iterations in 4.303s)\n",
      "[t-SNE] Iteration 600: error = 4.1624952, gradient norm = 0.0000037 (50 iterations in 4.218s)\n",
      "[t-SNE] Iteration 650: error = 4.1268724, gradient norm = 0.0000028 (50 iterations in 4.339s)\n",
      "[t-SNE] Iteration 700: error = 4.0977361, gradient norm = 0.0000023 (50 iterations in 4.333s)\n",
      "[t-SNE] Iteration 750: error = 4.0728608, gradient norm = 0.0000019 (50 iterations in 4.252s)\n",
      "[t-SNE] Iteration 800: error = 4.0504675, gradient norm = 0.0000016 (50 iterations in 4.285s)\n",
      "[t-SNE] Iteration 850: error = 4.0315836, gradient norm = 0.0000013 (50 iterations in 4.358s)\n",
      "[t-SNE] Iteration 900: error = 4.0147962, gradient norm = 0.0000012 (50 iterations in 4.343s)\n",
      "[t-SNE] Iteration 950: error = 3.9991454, gradient norm = 0.0000010 (50 iterations in 4.483s)\n",
      "[t-SNE] Iteration 1000: error = 3.9855384, gradient norm = 0.0000009 (50 iterations in 4.287s)\n",
      "[t-SNE] KL divergence after 1000 iterations: 3.985538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reducing tf-idf to dimensions\n",
      "done\n",
      "[t-SNE] Computing pairwise distances...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 1299\n",
      "[t-SNE] Computed conditional probabilities for sample 1299 / 1299\n",
      "[t-SNE] Mean sigma: 0.000000\n",
      "[t-SNE] Iteration 50: error = 51.0977086, gradient norm = 0.0432464 (50 iterations in 4.201s)\n",
      "[t-SNE] Iteration 100: error = 48.5515865, gradient norm = 0.0207282 (50 iterations in 4.583s)\n",
      "[t-SNE] Iteration 150: error = 48.1605306, gradient norm = 0.0161618 (50 iterations in 5.207s)\n",
      "[t-SNE] Iteration 200: error = 47.8889216, gradient norm = 0.0033309 (50 iterations in 4.653s)\n",
      "[t-SNE] Iteration 250: error = 47.6986835, gradient norm = 0.0027256 (50 iterations in 4.722s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 47.698683\n",
      "[t-SNE] Iteration 300: error = 4.8527009, gradient norm = 0.0003495 (50 iterations in 4.632s)\n",
      "[t-SNE] Iteration 350: error = 4.4764242, gradient norm = 0.0000404 (50 iterations in 4.865s)\n",
      "[t-SNE] Iteration 400: error = 4.3053874, gradient norm = 0.0000190 (50 iterations in 4.944s)\n",
      "[t-SNE] Iteration 450: error = 4.2034434, gradient norm = 0.0000103 (50 iterations in 4.866s)\n",
      "[t-SNE] Iteration 500: error = 4.1333534, gradient norm = 0.0000066 (50 iterations in 4.793s)\n",
      "[t-SNE] Iteration 550: error = 4.0786610, gradient norm = 0.0000048 (50 iterations in 4.869s)\n",
      "[t-SNE] Iteration 600: error = 4.0350535, gradient norm = 0.0000037 (50 iterations in 4.352s)\n",
      "[t-SNE] Iteration 650: error = 3.9969394, gradient norm = 0.0000030 (50 iterations in 4.511s)\n",
      "[t-SNE] Iteration 700: error = 3.9645015, gradient norm = 0.0000023 (50 iterations in 4.984s)\n",
      "[t-SNE] Iteration 750: error = 3.9386183, gradient norm = 0.0000019 (50 iterations in 4.527s)\n",
      "[t-SNE] Iteration 800: error = 3.9172218, gradient norm = 0.0000016 (50 iterations in 4.450s)\n",
      "[t-SNE] Iteration 850: error = 3.8983524, gradient norm = 0.0000013 (50 iterations in 4.244s)\n",
      "[t-SNE] Iteration 900: error = 3.8809446, gradient norm = 0.0000011 (50 iterations in 4.812s)\n",
      "[t-SNE] Iteration 950: error = 3.8645710, gradient norm = 0.0000010 (50 iterations in 5.428s)\n",
      "[t-SNE] Iteration 1000: error = 3.8481138, gradient norm = 0.0000011 (50 iterations in 4.499s)\n",
      "[t-SNE] KL divergence after 1000 iterations: 3.848114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reducing tf-idf to dimensions\n",
      "done\n",
      "[t-SNE] Computing pairwise distances...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 1299\n",
      "[t-SNE] Computed conditional probabilities for sample 1299 / 1299\n",
      "[t-SNE] Mean sigma: 0.000000\n",
      "[t-SNE] Iteration 50: error = 50.4018700, gradient norm = 0.0291540 (50 iterations in 4.453s)\n",
      "[t-SNE] Iteration 100: error = 47.8521143, gradient norm = 0.0172877 (50 iterations in 4.277s)\n",
      "[t-SNE] Iteration 150: error = 47.5506960, gradient norm = 0.0083787 (50 iterations in 4.630s)\n",
      "[t-SNE] Iteration 200: error = 47.2708540, gradient norm = 0.0043176 (50 iterations in 4.747s)\n",
      "[t-SNE] Iteration 250: error = 47.1051798, gradient norm = 0.0028086 (50 iterations in 4.770s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 47.105180\n",
      "[t-SNE] Iteration 300: error = 4.8738531, gradient norm = 0.0002819 (50 iterations in 5.047s)\n",
      "[t-SNE] Iteration 350: error = 4.4904312, gradient norm = 0.0000397 (50 iterations in 4.689s)\n",
      "[t-SNE] Iteration 400: error = 4.3134257, gradient norm = 0.0000213 (50 iterations in 4.318s)\n",
      "[t-SNE] Iteration 450: error = 4.2050118, gradient norm = 0.0000102 (50 iterations in 4.372s)\n",
      "[t-SNE] Iteration 500: error = 4.1290964, gradient norm = 0.0000067 (50 iterations in 4.353s)\n",
      "[t-SNE] Iteration 550: error = 4.0691735, gradient norm = 0.0000048 (50 iterations in 4.577s)\n",
      "[t-SNE] Iteration 600: error = 4.0188044, gradient norm = 0.0000040 (50 iterations in 4.457s)\n",
      "[t-SNE] Iteration 650: error = 3.9789413, gradient norm = 0.0000028 (50 iterations in 4.177s)\n",
      "[t-SNE] Iteration 700: error = 3.9440490, gradient norm = 0.0000023 (50 iterations in 4.163s)\n",
      "[t-SNE] Iteration 750: error = 3.9134170, gradient norm = 0.0000019 (50 iterations in 4.082s)\n",
      "[t-SNE] Iteration 800: error = 3.8857640, gradient norm = 0.0000016 (50 iterations in 4.409s)\n",
      "[t-SNE] Iteration 850: error = 3.8636333, gradient norm = 0.0000014 (50 iterations in 4.231s)\n",
      "[t-SNE] Iteration 900: error = 3.8439317, gradient norm = 0.0000012 (50 iterations in 4.750s)\n",
      "[t-SNE] Iteration 950: error = 3.8271657, gradient norm = 0.0000010 (50 iterations in 4.281s)\n",
      "[t-SNE] Iteration 1000: error = 3.8123494, gradient norm = 0.0000009 (50 iterations in 4.386s)\n",
      "[t-SNE] KL divergence after 1000 iterations: 3.812349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reducing tf-idf to dimensions\n",
      "done\n",
      "[t-SNE] Computing pairwise distances...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 1299\n",
      "[t-SNE] Computed conditional probabilities for sample 1299 / 1299\n",
      "[t-SNE] Mean sigma: 0.000000\n",
      "[t-SNE] Iteration 50: error = 50.2703857, gradient norm = 0.0334724 (50 iterations in 4.212s)\n",
      "[t-SNE] Iteration 100: error = 47.9990907, gradient norm = 0.0118225 (50 iterations in 4.155s)\n",
      "[t-SNE] Iteration 150: error = 47.4268128, gradient norm = 0.0041747 (50 iterations in 4.156s)\n",
      "[t-SNE] Iteration 200: error = 47.1097046, gradient norm = 0.0044691 (50 iterations in 4.261s)\n",
      "[t-SNE] Iteration 250: error = 47.1863817, gradient norm = 0.0025699 (50 iterations in 4.795s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 47.186382\n",
      "[t-SNE] Iteration 300: error = 5.0185799, gradient norm = 0.0002755 (50 iterations in 4.720s)\n",
      "[t-SNE] Iteration 350: error = 4.6544473, gradient norm = 0.0000401 (50 iterations in 4.688s)\n",
      "[t-SNE] Iteration 400: error = 4.4773893, gradient norm = 0.0000176 (50 iterations in 4.144s)\n",
      "[t-SNE] Iteration 450: error = 4.3677922, gradient norm = 0.0000106 (50 iterations in 4.095s)\n",
      "[t-SNE] Iteration 500: error = 4.2896110, gradient norm = 0.0000067 (50 iterations in 4.120s)\n",
      "[t-SNE] Iteration 550: error = 4.2320618, gradient norm = 0.0000046 (50 iterations in 4.123s)\n",
      "[t-SNE] Iteration 600: error = 4.1838872, gradient norm = 0.0000035 (50 iterations in 4.112s)\n",
      "[t-SNE] Iteration 650: error = 4.1470845, gradient norm = 0.0000028 (50 iterations in 4.106s)\n",
      "[t-SNE] Iteration 700: error = 4.1157255, gradient norm = 0.0000022 (50 iterations in 4.128s)\n",
      "[t-SNE] Iteration 750: error = 4.0877741, gradient norm = 0.0000018 (50 iterations in 4.115s)\n",
      "[t-SNE] Iteration 800: error = 4.0630588, gradient norm = 0.0000015 (50 iterations in 4.121s)\n",
      "[t-SNE] Iteration 850: error = 4.0407610, gradient norm = 0.0000013 (50 iterations in 4.222s)\n",
      "[t-SNE] Iteration 900: error = 4.0214666, gradient norm = 0.0000011 (50 iterations in 4.247s)\n",
      "[t-SNE] Iteration 950: error = 4.0035247, gradient norm = 0.0000010 (50 iterations in 4.172s)\n",
      "[t-SNE] Iteration 1000: error = 3.9866297, gradient norm = 0.0000009 (50 iterations in 4.141s)\n",
      "[t-SNE] KL divergence after 1000 iterations: 3.986630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reducing tf-idf to dimensions\n",
      "done\n",
      "[t-SNE] Computing pairwise distances...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 1299\n",
      "[t-SNE] Computed conditional probabilities for sample 1299 / 1299\n",
      "[t-SNE] Mean sigma: 0.000000\n",
      "[t-SNE] Iteration 50: error = 50.7266963, gradient norm = 0.0376141 (50 iterations in 4.139s)\n",
      "[t-SNE] Iteration 100: error = 48.1241411, gradient norm = 0.0170083 (50 iterations in 4.142s)\n",
      "[t-SNE] Iteration 150: error = 47.6962859, gradient norm = 0.0065613 (50 iterations in 4.107s)\n",
      "[t-SNE] Iteration 200: error = 47.3383434, gradient norm = 0.0025313 (50 iterations in 4.110s)\n",
      "[t-SNE] Iteration 250: error = 47.1295317, gradient norm = 0.0042342 (50 iterations in 4.119s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 47.129532\n",
      "[t-SNE] Iteration 300: error = 4.8515285, gradient norm = 0.0002839 (50 iterations in 4.069s)\n",
      "[t-SNE] Iteration 350: error = 4.4707891, gradient norm = 0.0000399 (50 iterations in 4.172s)\n",
      "[t-SNE] Iteration 400: error = 4.3030286, gradient norm = 0.0000176 (50 iterations in 4.231s)\n",
      "[t-SNE] Iteration 450: error = 4.1989293, gradient norm = 0.0000101 (50 iterations in 4.160s)\n",
      "[t-SNE] Iteration 500: error = 4.1228895, gradient norm = 0.0000067 (50 iterations in 4.151s)\n",
      "[t-SNE] Iteration 550: error = 4.0640455, gradient norm = 0.0000054 (50 iterations in 4.431s)\n",
      "[t-SNE] Iteration 600: error = 4.0181997, gradient norm = 0.0000037 (50 iterations in 4.424s)\n",
      "[t-SNE] Iteration 650: error = 3.9801234, gradient norm = 0.0000030 (50 iterations in 4.218s)\n",
      "[t-SNE] Iteration 700: error = 3.9488251, gradient norm = 0.0000023 (50 iterations in 4.097s)\n",
      "[t-SNE] Iteration 750: error = 3.9215692, gradient norm = 0.0000020 (50 iterations in 4.149s)\n",
      "[t-SNE] Iteration 800: error = 3.8974051, gradient norm = 0.0000016 (50 iterations in 4.068s)\n",
      "[t-SNE] Iteration 850: error = 3.8776230, gradient norm = 0.0000013 (50 iterations in 4.109s)\n",
      "[t-SNE] Iteration 900: error = 3.8601321, gradient norm = 0.0000012 (50 iterations in 4.218s)\n",
      "[t-SNE] Iteration 950: error = 3.8435456, gradient norm = 0.0000010 (50 iterations in 4.203s)\n",
      "[t-SNE] Iteration 1000: error = 3.8284100, gradient norm = 0.0000009 (50 iterations in 4.113s)\n",
      "[t-SNE] KL divergence after 1000 iterations: 3.828410\n"
     ]
    }
   ],
   "source": [
    "for x in range(1,11):\n",
    "    tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english', min_df=0.025, max_df=.5) #NOTE\n",
    "    # this step takes longest & contains lots of important parameters; playing with these and experimenting\n",
    "    # with them is recommended. Starting here::\n",
    "    # https://www.kaggle.com/adamschroeder/countvectorizer-tfidfvectorizer-predict-comments\n",
    "    # and moving on to the official docs here:\n",
    "    # http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer\n",
    "    # is a good idea.\n",
    "\n",
    "    tfs = tfidf.fit_transform(tempDict.values())\n",
    "    print(\"reducing tf-idf to dimensions\")\n",
    "    tfs_reduced = TruncatedSVD(n_components=10, random_state=0).fit_transform(tfs)\n",
    "    print(\"done\")\n",
    "    \n",
    "    model = TSNE(n_components=10, perplexity=5, verbose=2, method='exact').fit_transform(tfs_reduced)\n",
    "\n",
    "    # save to json file\n",
    "    x_axis=model[:,0]\n",
    "    y_axis=model[:,1]\n",
    "    x_norm = (x_axis-np.min(x_axis)) / (np.max(x_axis) - np.min(x_axis))\n",
    "    y_norm = (y_axis-np.min(y_axis)) / (np.max(y_axis) - np.min(y_axis))\n",
    "    data = {\"x\":x_norm.tolist(), \"y\":y_norm.tolist(), \"names\":tempDict.keys()} #output x and y coords in data\n",
    "    \n",
    "    \n",
    "\n",
    "    test_df_coords = pd.DataFrame(data)[['x','y']].reset_index()\n",
    "    test_df_coords.columns = ['names','x','y']\n",
    "\n",
    "    result = pd.merge(df_stack, test_df_coords, left_on='title', right_on='names')\n",
    "    filename = 'oreos_tsn_reduced_sample_v2_'+str(x)+'.csv'\n",
    "    result.to_csv(filename)\n",
    "    \n",
    "    del tfidf\n",
    "    del tfs\n",
    "    del tfs_reduced\n",
    "    del x_axis\n",
    "    del y_axis\n",
    "    del x_norm\n",
    "    del y_norm\n",
    "    del data\n",
    "    del test_df_coords\n",
    "    del result\n",
    "    del filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "t-SNE for Thomas.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
